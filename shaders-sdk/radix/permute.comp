#version 460 core
#extension GL_GOOGLE_include_directive : enable

#include "./includes.glsl"

layout (local_size_x = BLOCK_SIZE) in;

shared uint localHistogram[RADICES];

// shared for 16 threads (with lanes)
shared URDC_WARP _data[WARP_SIZE];
#define key _data[LANE_IDX]

initNonAtomicIncFunctionTarget(localHistogram[WHERE], countOffset, uint)

void main() {
    LT_IDX = gl_LocalInvocationID.x;
    LF_IDX = gl_LocalInvocationID.x / WARP_SIZE_RT;
    LC_IDX = gl_WorkGroupID.y + LF_IDX * gl_WorkGroupSize.y;
    LANE_IDX = (gl_LocalInvocationID.x % WARP_SIZE_RT);

    // set prefix sum (planned distribute threads) 
    for (uint rk=0;rk<RADICES;rk+=WRK_SIZE_RT) {
        UVEC_WARP radice = UVEC_WARP(rk + LC_IDX);
        localHistogram[radice] = PrefixSum[radice + gl_WorkGroupID.x * RADICES];
    }
    memoryBarrier(); barrier();

    // calculate blocks
    blocks_info blocks = get_blocks_info(NumKeys);
    uint bcount = min(blocks.count, 16384);
    WPTR addr = WPTR(blocks.offset) + WPTR(LANE_IDX);
    for ( uint wk = 0; wk < bcount; wk++ ) {
        BVEC_WARP validAddress = addr < NumKeys;
        IFALL(!validAddress) break;

        //memoryBarrier(); barrier();
        if (LF_IDX == 0) {
            key = URDC_WARP(BFE(validAddress ? KeyTmp[addr] : KEYTYPE(0xFFFFFFFFFFFFFFFFul), Shift*BITS_PER_PASS, BITS_PER_PASS));
        }
        memoryBarrier(); barrier();

        // WARP-optimized histogram calculation
        for (uint rk=0;rk<RADICES;rk+=WRK_SIZE_RT) {
            UVEC_WARP radice = UVEC_WARP(rk + LC_IDX);
            bool owned = key == radice && validAddress;
            if (owned) {
                WPTR offset = WPTR(countOffset(radice, TRUE_)); 
                KeyIn[offset] = KeyTmp[addr], ValueIn[offset] = ValueTmp[addr];
            }
            IFALL (radice >= RADICES || owned || !validAddress) break;
        }

        addr += WARP_SIZE_RT;
    }
}
