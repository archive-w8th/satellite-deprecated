#version 460 core
#extension GL_GOOGLE_include_directive : enable

#include "./includes.glsl"

layout (local_size_x = BLOCK_SIZE) in;

shared uint localHistogram[RADICES];

// shared for 16 threads (with lanes)
#ifdef ENABLE_AMD_INSTRUCTION_SET
shared URDC_WARP_LCM _data[64];
#else
shared mediump URDC_WARP_LCM _data[128];
#endif

#define key _data[LANE_IDX]

//initSubgroupIncFunctionTarget(localHistogram[WHERE], countOffset, 1, uint)

#ifdef ENABLE_AMD_INSTRUCTION_SET
initSubgroupIncFunctionTargetDual(localHistogram[WHERE], countOffset, 1, uint, uvec2)
#else
initSubgroupIncFunctionTarget(localHistogram[WHERE], countOffset, 1, uint)
#endif

void main() {
    LT_IDX = gl_LocalInvocationID.x;
    LF_IDX = gl_LocalInvocationID.x / WARP_SIZE_RT;
    LC_IDX = gl_WorkGroupID.y + LF_IDX * gl_WorkGroupSize.y;
    LANE_IDX = (gl_LocalInvocationID.x % WARP_SIZE_RT);

    // set prefix sum (planned distribute threads) 
    [[unroll]]
    for (uint rk=0;rk<RADICES;rk+=WRK_SIZE_RT) {
        UVEC_WARP radice = UVEC_WARP(rk + LC_IDX);
        localHistogram[radice] = PrefixSum[radice + gl_WorkGroupID.x * RADICES];
    }
    
    LGROUP_BARRIER

    // calculate blocks
    blocks_info blocks = get_blocks_info(NumKeys);

#ifdef ENABLE_AMD_INSTRUCTION_SET
    uint bcount = min(tiled(blocks.count, 2u), 65536u);
    WPTR2 addr = WPTR(blocks.offset).xx + WPTR2(LANE_IDX, WARP_SIZE_RT + LANE_IDX);
#else
    uint bcount = min(blocks.count, 65536u);
    WPTR addr = WPTR(blocks.offset) + WPTR(LANE_IDX);
#endif

    for ( uint wk = 0; wk < bcount; wk++ ) {
#ifdef ENABLE_AMD_INSTRUCTION_SET
        BVEC2_WARP validAddress = lessThan(addr, blocks.limit.xx);
        IFALL(all(not(validAddress))) break;
#else
        BVEC_WARP validAddress = addr < blocks.limit;
        IFALL(!validAddress.x) break;
#endif

        if (LF_IDX == 0) {
#ifdef ENABLE_AMD_INSTRUCTION_SET
            key = packUint2x16(URDC_WARP_DUAL(
                BFE(validAddress.x ? KeyTmp[addr.x] : KEYTYPE(0xFFFFFFFFu.xx), Shift*BITS_PER_PASS, BITS_PER_PASS),
                BFE(validAddress.y ? KeyTmp[addr.y] : KEYTYPE(0xFFFFFFFFu.xx), Shift*BITS_PER_PASS, BITS_PER_PASS)
            ));
#else
            key.x = URDC_WARP(BFE(validAddress.x ? KeyTmp[addr.x] : KEYTYPE(0xFFFFFFFFu.xx), Shift*BITS_PER_PASS, BITS_PER_PASS));
#endif
        }
        LGROUP_BARRIER

        // WARP-optimized histogram calculation
        for (uint rk=0u;rk<RADICES;rk+=WRK_SIZE_RT) {
            UVEC_WARP radice = UVEC_WARP(rk + LC_IDX);
#ifdef ENABLE_AMD_INSTRUCTION_SET
            bvec2 owned = and(equal(unpackUint2x16(key), radice.xx), validAddress);
            if (any(owned)) {
                WPTR2 offset = WPTR2(countOffset(uint(radice), owned));
                if (owned.x) { KeyIn[offset.x] = KeyTmp[addr.x], ValueIn[offset.x] = ValueTmp[addr.x]; }
                if (owned.y) { KeyIn[offset.y] = KeyTmp[addr.y], ValueIn[offset.y] = ValueTmp[addr.y]; }
            }
            IFALL (all(or((radice >= RADICES).xx, or(owned, not(validAddress))))) break;
#else
            bool owned = key.x == radice && validAddress;
            if (owned) {
                WPTR offset = WPTR(countOffset(uint(radice))); 
                KeyIn[offset.x] = KeyTmp[addr.x], ValueIn[offset.x] = ValueTmp[addr.x];
            }
            IFALL (radice >= RADICES || owned || !validAddress) break;
#endif
        }

#ifdef ENABLE_AMD_INSTRUCTION_SET
        addr += WARP_SIZE_RT*2;
#else
        addr += WARP_SIZE_RT;
#endif
    }
}
