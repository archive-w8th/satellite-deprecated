#version 460 core
#extension GL_GOOGLE_include_directive : enable

#include "./includes.glsl"

layout (local_size_x = BLOCK_SIZE) in;

shared uint localHistogram[RADICES];

// shared for 16 threads (with lanes)
#ifdef ENABLE_AMD_INSTRUCTION_SET
shared URDC_WARP_LCM _data[64];
#else
shared mediump URDC_WARP_LCM _data[128];
#endif

#define key _data[LANE_IDX]

#ifdef ENABLE_AMD_INSTRUCTION_SET
initSubgroupIncFunctionTargetDual(localHistogram[WHERE], countHistogram, 1, uint, uvec2)
#else
initSubgroupIncFunctionTarget(localHistogram[WHERE], countHistogram, 1, uint)
#endif

void main() {
    LT_IDX = gl_LocalInvocationID.x;
    LF_IDX = gl_LocalInvocationID.x / WARP_SIZE_RT;
    LC_IDX = gl_WorkGroupID.y + LF_IDX * gl_WorkGroupSize.y;
    LANE_IDX = (gl_LocalInvocationID.x % WARP_SIZE_RT);

    // clear histogram of block (planned distribute threads)
    [[unroll]]
    for (uint rk=0;rk<RADICES;rk+=WRK_SIZE_RT) {
        localHistogram[rk + LC_IDX] = 0;
    }
    GROUP_BARRIER

    // use SIMD lanes for calculate histograms
    blocks_info blocks = get_blocks_info(NumKeys);

#ifdef ENABLE_AMD_INSTRUCTION_SET
    uint bcount = min(tiled(blocks.count, 2u), 65536u);
    WPTR2 addr = WPTR(blocks.offset).xx + WPTR2(LANE_IDX, WARP_SIZE_RT + LANE_IDX);
#else
    uint bcount = min(blocks.count, 65536u);
    WPTR addr = WPTR(blocks.offset) + WPTR(LANE_IDX);
#endif

    for ( uint wk = 0; wk < bcount; wk++ ) {
#ifdef ENABLE_AMD_INSTRUCTION_SET
        BVEC2_WARP validAddress = lessThan(addr, blocks.limit.xx);
        IFALL(all(not(validAddress))) break;
#else
        BVEC_WARP validAddress = addr < blocks.limit;
        IFALL(!validAddress.x) break;
#endif

        if (LF_IDX == 0) {
#ifdef ENABLE_AMD_INSTRUCTION_SET
            key = packUint2x16(URDC_WARP_DUAL(
                BFE(validAddress.x ? KeyTmp[addr.x] : KEYTYPE(0xFFFFFFFFu.xx), Shift*BITS_PER_PASS, BITS_PER_PASS),
                BFE(validAddress.y ? KeyTmp[addr.y] : KEYTYPE(0xFFFFFFFFu.xx), Shift*BITS_PER_PASS, BITS_PER_PASS)
            ));
#else
            key.x = URDC_WARP(BFE(validAddress.x ? KeyTmp[addr.x] : KEYTYPE(0xFFFFFFFFu.xx), Shift*BITS_PER_PASS, BITS_PER_PASS));
#endif
        }
        GROUP_BARRIER

        // WARP-optimized histogram calculation
        for (uint rk=0u;rk<RADICES;rk+=WRK_SIZE_RT) {
            UVEC_WARP radice = UVEC_WARP(rk + LC_IDX);
#ifdef ENABLE_AMD_INSTRUCTION_SET
            bvec2 owned = and(equal(unpackUint2x16(key), radice.xx), validAddress);
            if (any(owned)) countHistogram(uint(radice), owned);
            IFALL (all(or((radice >= RADICES).xx, or(owned, not(validAddress))))) break;
#else
            bool owned = key.x == radice && validAddress.x;
            if (owned) countHistogram(uint(radice));
            IFALL (radice >= RADICES || owned || !validAddress.x) break;
#endif
        }

#ifdef ENABLE_AMD_INSTRUCTION_SET
        addr += WARP_SIZE_RT*2;
#else
        addr += WARP_SIZE_RT;
#endif
    }

    // resolve histograms (planned distribute threads) 
    GROUP_BARRIER

    [[unroll]]
    for (uint rk=0;rk<RADICES;rk+=WRK_SIZE_RT) {
        UVEC_WARP radice = UVEC_WARP(rk + LC_IDX);
        if (radice < RADICES) {
            PrefixSum[radice + RADICES * gl_WorkGroupID.x] = localHistogram[radice];
            Histogram[radice + RADICES * gl_WorkGroupID.x] = localHistogram[radice];
        }
    }
}
